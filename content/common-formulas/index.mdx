---
title: "Leetcode: Common Space-Time Complexity Calucations"
description: "Common Space-Time Calucations"
image: "../../public/blogs/chalk_board_1.png"
publishedAt: "2024-08-04"
updatedAt: "2025-05-22" 
author: "jonathancamberos"
isPublished: true
tags: 
- data structures & algorithms
---

## Brute Case Double For Loop
```python
    def example(self, nums: List[int]) -> bool:

        # compare every element, with every element

        for i in range(len(nums)):
            for j in range(i + 1, len(nums)):
                if nums[i] == nums[j]:
                    return True

        return False
```

### Time Complexity Derivation
|  Outer Loop (i) | Inner Loop Start: (j = i + 1) | Inner Loop End: (j) |  Total Inner Loop Iterations | Total Outer Loop Iterations (Remaining) |
| --------------- | ----------------------------- | ------------------- | ---------------------------- | --------------------------- |
| i = 0 | j = 1 | j = n - 1 |  n - 1 |  n |
| i = 1 | j = 2 | j = n - 1 |  n - 2 |  n - 1 | 
| i = 2 | j = 3 | j = n - 1 |  n - 3 |  n - 2 | 
| ...   | ... | ... | ... | ... |
| i = n - 2 |  j = n - 1 | j = n - 1 | 1 | 2 |
| i = n - 1 |  j = n | None (out of bounds) | 0 | 1 |
| i = n |  j = n + 1 | None (out of bounds) | 0 | 0 |

The total number of iterations in the inner loop across each iteration of the outer loop is:
(n − 1) + (n − 2) + (n − 3) + ... + 3 + 2 + 1

We can write this series backwards: 
1 + 2 + 3 + ... + (n - 3) + (n - 2) + (n − 1)

Now if we add 2 of the same series:

[Forward series] + [Backward series] = 

[(n − 1) + (n − 2) + (n − 3) + ... + 3 + 2 + 1] + [1 + 2 + 3 + ... + (n - 3) + (n - 2) + (n − 1)] =

We can merge the same pairs:

[(n − 1) + 1, (n − 2) + 2,(n − 3) + 3, ... ] =

Each individual pairs cancel out n, and we left n-1 number of pairs. So the sum of the two series is:

n * (n-1)

And since we added the series twice, divide by 2 we get: 

[n * (n-1)] / 2

Now if we simplify and apply Big(O) then we arrive at O(n<sup>2</sup>) time complexity.

## Merge Sort
Merge Sort is a divide and conquer sorting algorithm that sorts an array by recursion:

1. Given an array. Split into two halves
2. Split until you get base case of list of 1 element per list
3. Recurse, merging and sorting 2 lists at a time
4. Return to original length now sorted list

### Merge Sort Diagram
```
Splitting Phase:

Level 0:       [8, 4, 7, 3, 2, 6, 5, 1]        2^0 = 1 subarrays  (n elements, O(n))
                   /              \
Level 1:    [8, 4, 7, 3]      [2, 6, 5, 1]     2^1 = 2 subarrays  (n/2 elements, O(n/2))
               /     \          /     \
Level 2:    [8, 4] [7, 3]     [2, 6] [5, 1]    2^2 = 4 subarrays  (n/4 elements, O(n/4))
             /  \    /   \     /   \   /  \
Level 3:    [8] [4] [7] [3]   [2] [6] [5] [1]  2^3 = 8 subarrays  (n/8 element, O(1))
```

+ In merge sort, the splitting does not contribute to O(n) per level, since we simply pass indices indicating bounds of the subarray which is O(1).
+ Recursive Memory Lifecycle:


```
Merging Phase:

Level 3:   [8] [4]   [7] [3]    [2] [6]   [5] [1]   (base case, single elements)
            \  /      \  /        \  /      \  /
Level 2:   [4, 8]   [3, 7]       [2, 6]   [1, 5]    (merge and sort pairs)
               \       /             \       /
Level 1:     [3, 4, 7, 8]         [1, 2, 5, 6]      (merge sorted halves)
                   \                  /
Level 0:        [1, 2, 3, 4, 5, 6, 7, 8]            (final merge)

```

+ In merge sort, the merging step is where all the real work happens.
+ At each level, you have 2<sup>level</sup> subarrays.
+ Each subarray's size of n/2<sup>level</sup>
+ Total merge work done at each level = num of subarrays * size per subarray = 2<sup>level</sup> * n/2<sup>level</sup> = n
+ Total merge work done = merge work at each level * num of levels = n * log(n) = O(n log n)


```
Recursion Phase 1:

Level 0:       [8, 4, 7, 3, 2, 6, 5, 1]        2^0 = 1 subarrays  (n elements, O(n))
                   /            \     
Level 1:    [8, 4, 7, 3]     [2, 6, 5, 1]      2^1 = 2 subarrays  (n/2 elements, O(n/2)) 
               /                      
Level 2:    [8, 4]                             2^2 = 4 subarrays  (n/4 elements, O(n/4))
             /  \                      
Level 3:    [8] [4]                            2^3 = 8 subarrays  (n/8 element, O(1))
```

+ We can also go step by step to demonstrate why space complexity is O(n)

```
Recursion Phase 2:

Level 0:       [8, 4, 7, 3, 2, 6, 5, 1]        2^0 = 1 subarrays  (n elements, O(n))
                   /             \ 
Level 1:    [8, 4, 7, 3]     [2, 6, 5, 1]      2^1 = 2 subarrays  (n/2 elements, O(n/2)) 
               /     \               
Level 2:    [4, 8] [7, 3]                      2^2 = 4 subarrays  (n/4 elements, O(n/4))
                    /   \             
Level 3:           [7] [3]                     2^3 = 8 subarrays  (n/8 element, O(1))
```

```
Recursion Phase 3:

Level 0:       [8, 4, 7, 3, 2, 6, 5, 1]        2^0 = 1 subarrays  (n elements, O(n))
                   /             \ 
Level 1:    [8, 4, 7, 3]     [2, 6, 5, 1]      2^1 = 2 subarrays  (n/2 elements, O(n/2)) 
               /     \               
Level 2:    [4, 8] [3, 7]                      2^2 = 4 subarrays  (n/4 elements, O(n/4))
                                      
Level 3:                                       2^3 = 8 subarrays  (n/8 element, O(1))
```

```
Recursion Phase 4:

Level 0:       [8, 4, 7, 3, 2, 6, 5, 1]        2^0 = 1 subarrays  (n elements, O(n))
                   /              \
Level 1:    [3, 4, 7, 8]     [2, 6, 5, 1]      2^1 = 2 subarrays  (n/2 elements, O(n/2))                  2^1 = 2 subarrays  (n/2 elements, O(n/2))
                                   
Level 2:                                       2^2 = 4 subarrays  (n/4 elements, O(n/4))
                                      
Level 3:                                       2^3 = 8 subarrays  (n/8 element, O(1))
```

+ Notice that how peak memory usage at any moment involves only a few active arrays 
sums roughly to n.
+ Therefore, memory is not cumulative across all calls, but localized to active recursive paths


### Merge Sort Code
```python
    def isAnagram(self, s: str, t: str) -> bool:
        if len(s) != len(t):
            return False
        
        # Splitting list and recursively sorting
        def mergeSort(string):
            if len(string) <= 1:
                return string
            
            mid = len(string) // 2
            left = mergeSort(string[:mid])
            right = mergeSort(string[mid:])
            
            return merge(left, right)
        
        # merging and sorting left + right sorted arrays
        def merge(left, right):
            sorted = []
            i = j = 0
            while i < len(left) and j < len(right):
                if left[i] <= right[j]:
                    sorted.append(left[i])
                    i += 1
                else:
                    sorted.append(right[j])
                    j += 1
            
            # Explicitly handle any remaining elements
            if i < len(left):   # If elements remain in `left`
                sorted.extend(left[i:])
            else:               # If elements remain in `right`
                sorted.extend(right[j:])
            return sorted

        # Sorting both s and t strings and comparing
        sorted_s = mergeSort(list(s))
        sorted_t = mergeSort(list(t))

        return sorted_s == sorted_t
```

### Space & Time Complexity
|  Aspect  | Time Complexity | Space Complexity | Time Explanation | Space Explanation |  
| -------- | --------------- | ---------------- | ---------------- | ----------------- |
| Dividing the Array | O(log n) | O(log n) | The recursive functional calls use stack memory proportional to the depth of recursion, which is O(log n)| |
| Merging at 1 level | O(n) | O(n) | A merge operation at a single level combines two sorted subarrays, by iterating over both sorted arrays and adding elements in order to the result array. For a merge at level l, the work is (num of subarrays * work per subarray) = O(2<sup>t</sup> * n/2<sup>t</sup>) = O(n) | .   |
| Merging across all levels | O(n log n) | O(n) | Over log n levels of recursion, each level processes n elements, leading to O(n log n) | While we do create arrays of size n/4, n/8, and so forth, there will only actually exist two of such arrays at any given time. For example, we start by creating two arrays of size n/2. Then, we recurse down only one one of the arrays and create two arrays of size n/4. Then we recurse down again and create two arrays of size n/8... And once we return from a recursive call, we end up destroying the temporary arrays we created in that call. |
| Overall | O(n log n) | O(n) | Merging across all levels dominates time complexity. Leading to O(n log n) | Memory allocation for temporary arrays during merging at each level dominate space complexity. Leading to O(n) |  |  |

### Best, Average, Worst Case

Time Complexity of merge sort is O(n log n) across the all 3 cases. 
This is because Merge sort still recursively divides and merges the array, regardless if the initial array is sorted or unsorted.  
So it still goes through all the steps and requirements of dividing and merging across all levels leading to the same time and space complexity for best, average, and worst case.

### Summary 
Merge sort's behavior does not depend on the input order.
It is a predicable sorting algorithm with its main drawback being the space requirement O(n) when
compared to in-place sorting algorithms like quicksort or heap sort.