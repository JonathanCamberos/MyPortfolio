---
title: "LeetCode: Arrays and Hashing"
description: "arrays & hashing"
image: "../../public/Notes/hashmap.png"
publishedAt: "2025-06-13"
updatedAt: "2025-06-26"
author: "jonathancamberos"
isPublished: true
tags:
- data structures and algorithms
---

# Arrays & Hashing Intro:

LeetCode problems with solutions using hashmaps. 

## What is a Hashmap
A dictionary is just a direct mapping of keys->values:

Dictionary: ['a': 2, 'b': 2, 'c' : 3, ...] 

A Hashmap is just a dictionary that maps keys->values using a hash function.
Hashmaps are a common use case for hashing.
We choose hashing to maximize randomness and minimize collisions between elements 
to keys.

## Hashing With A Function
Hashing is simply a function. It takes in an input and spits out an output.
Here is the function mod, which takes in an integer and spits out an integer:

- 1 mod (3) = 1
- 2 mod (3) = 2
- 3 mod (3) = 0
- 4 mod (3) = 1

Now this is could be our function for our hashmap, but it would lead to high 
collisions and low randomness as there are only 3 possible key 
results: 0, 1, and 2. So hashmaps are only efficient as the chosen hash function.

## Choosing a Hash Function

With a good hash function:
Insert, Lookup, and Delete take O(1).
In this case, every element gets its own unique key and so
a lookup using this function would be constant O(1).

With a bad hash function:
Insert, Lookup, and Delete take O(n). 
Lets say our function is hash():

## Handling Collisions
Even with a good hash function, we may run into collisions sometimes.
In those cases, we handle collision using chaining with a linked list of elements.

Here, there are 5 keys or buckets we could hash to.

```python
    hash()  ->  [key mod (5)]
    ----------------------------
    hash(10) = 10 % 5 = 0
    hash(22) = 22 % 5 = 2
    hash(31) = 31 % 5 = 1
    hash(14) = 14 % 5 = 4
    hash(17) = 17 % 5 = 2 (collision!)
```

| Key (index) | Value (element) |
| ----------- | --------------- |
| 0           | [10]            | 
| 1           | [31]            |
| 2           | [22, 17]        |
| 3           | []              |
| 4           | [14]            |

## Balancing Hash Table With Load Factor
Load factors are calculated to ensure a hash table maintains its O(1) time complexity.

Balancing occurs when a hash table has passed its set load factor.

Load Factor = n/m
Where n = num of elements, m = num of keys

Thus, when certain fraction of the table size, say 75% full with 3 elements and 4 potential keys. 
The table must increase in size and rehash/reinsert every element. 

The efficiency of a hash table decreases significantly without balancing, as this 
leads to increased collisions as the table fills up, which leads to time needed to 
resolve collisions, as we traverse through list to find the element in linear 
time O(n). Operations like search, insert, and delete, degrade from O(1) on average 
to O(n) in the worst case.

Usually, when the load factor is reached, the table size doubles to 2n.
0.75 or 75% full is a common load factor for a hash table.

Upon balancing, we need to rehash every element leading to n/2 additional space:
```python
    New table size - current number of elements 
    (double table size * load factor) - current n elements
    (2 n * 0.75) 
    1.5 n  = New Load Factor Element Count

    New Load Factor Element Count - Current Element Count
    1.5 n - n 
    0.5
    n/2 = Additional space until next rebalance
```
Rehashing is expensive as we need to rehash every element with the new hash function
into their new bucket, leading to O(n) for resizing and reinserting n elements.

## Array Application: In place Transformations
We can perform transformations or reorderings on the array 
itself without using extra space.

Ex: Rotate an array to the right by k steps.
```python
    def rotate(nums: List[int], k: int) -> None:
        
        n = len(nums)
        k %= n  # handle cases where k > n
        
        # Opposite ends reverse subarray [start, end]
        def reverse(start: int, end: int) -> None:
            
            # Reverse until hit middle of subarray
            while start < end:
                nums[start], nums[end] = nums[end], nums[start]
                start += 1
                end -= 1

        # Reverse entire array
        reverse(0, n - 1)
        # Reverse first k elements [0, k)
        reverse(0, k - 1)
        # Reverse remaining elements [k, n)
        reverse(k, n - 1)
```

## HashMap Application: Representations
We can represent objects or data based on specific criteria.

Ex: Representing a string by character frequency
```python
    def freqCount():

        # object
        s = "aabbcc"

        # representation
        freq = {}

        # mapping
        for char in s:
            freq[char] = freq.get(char, 0) + 1

    # freq = {'a': 2, 'b': 2, 'c': 2}
```

## HashMap Application: Grouping by Criteria
We can group elements based on a defined criterion, 
such as sorting or categorization, using hashing to push
values into the corresponding bucket.

Ex: Grouping string anagrams
```python
    def groupAnagrams(strs):
        
        # groups
        anagrams = {}

        # for list
        for word in strs:

            # create key
            key = "".join(sorted(word))
            if key not in anagrams:
                anagrams[key] = []
            
            # hash key and put value into bucket
            anagrams[key].append(word)

    # anagrams = {'aet': ['eat', 'tea', 'ate'], 'ant': ['tan', 'nat'], 'abt': ['bat']}
```

## HashMap Application: Memoization in Dynamic Programming
We can store solutions to sub problems to avoid redundant calculations.

Ex: Fibonacci number computation with memoization
```python
    def fib(n, memo={}):
        if n <= 1:
            return n
        if n not in memo:
            memo[n] = fib(n - 1, memo) + fib(n - 2, memo)
        return memo[n]
```

## HashMap Application: Backtracking with Caching for Pruning
We can cache previous paths or states to prune the search
space effectively as we explore.

Ex: Subset Sum with caching:
```python
    def subsetSum(nums, target):

        result = []

        # Cache to avoid exploring previously visited paths
        cache = {}

        def dfs_backtrack(current, index, total):
            
            # Check if already explored state
            state = (tuple(current), total)
            if state in cache:
                return

            # Mark as explored
            cache[state] = True

            # Early Prune -> no potential valid path is current total exceeds target
            if total > target:
                return
            
            # Valid solution -> no need to explore further, backtrack
            if total == target:
                result.append(list(current))
                return

            # Explore -> branches from current state
            for i in range(index, len(nums)):
                
                # Build
                current.append(nums[i])
                # Explore
                dfs_backtrack(current, i + 1, total + nums[i])
                # Backtrack
                current.pop()

        dfs_backtrack([], 0, 0)
        return result
```

## HashMap Application: Representing Relationships 
We can model relationships between entities.

Ex: Adjacency list for a graph
```python
    def graph():

        # List of edges in the graph
        edges = [(1, 2), (2, 3), (1, 3)]

        # Adjacency list
        graph = {}

        # Build the adjacency list
        for u, v in edges:

            # If the node 'u' or 'v' is not yet in the graph, initialize it
            if u not in graph:
                graph[u] = []
            if v not in graph:
                graph[v] = []

            # Add 'v' to the list of neighbors for 'u' and 'u' to 'v'
            graph[u].append(v)
            graph[v].append(u)

    # graph = {1: [2, 3], 2: [1, 3], 3: [2, 1]}
```

## HashMap Application: Index with Data Key
We can index into arrays based on data or data structures.

Ex: Store integer complements for quick lookup:
```python
    def twoSum(nums, target):

        # Complement -> index
        complement_map = {}

        for i, num in enumerate(nums):
            
            # Calculate complement
            complement = target - num
            
            # Lookup(complement)
            if num in complement_map:
                return [complement_map[num], i]
            
            # Put(complement)
            complement_map[complement] = i

        return []
```

## HashMap Application: Algorithm
There are cases where problem that seems to require a hashmap 
have an existing algorithm made for that problem.

Ex: Boyer Moore Voting Algorithm
```python
def majorityElement(nums: List[int]) -> int:
    
    # Find element that appears more then floor(n/2) times

    # votes for candidate
    count = 0

    # current candidate
    candidate = None

    for num in nums:
        
        # Reset candidate
        if count == 0:
            candidate = num

        if num == candidate:
            count += 1
        else:
            count -= 1

    # Confirm the candidate ( optional if majority element is guaranteed)
    return candidate
```


# 217. Contains Duplicate ::2:: - Easy

Topics:  Array, Hash Table, Sorting 

## Intro
> Given an integer array nums, 
> return true if any value appears at least twice in the array, 
> return false if every element is distinct.

|  Example Input           | Output |  
| ---------------- | ------ | 
| nums = [1,2,3,1] | true   |
| nums = [1,2,3,4] | false  |  
| nums = [1,1,1,3,3,4,3,2,4,2] | true |
 
Constraints:

1 &le; nums.length &le; 10<sup>5</sup>

-10<sup>9</sup> &le; nums[i] &le; 10<sup>9</sup> 


## Abstraction
Given a list of elements, check for duplicates.

## Space & Time Complexity
|  Solution  | Time Complexity | Space Complexity | Time Remark | Space Remark |  
| ---------- | --------------- | ---------------- | ----------- | ------------ |
| Hashmap | O(n) | O(n) | Iterate over array O(n) + get, put in O(1) | Dictionary size relative to input O(n) |
| Set()   | O(n) | O(n) | Iterate over array O(n) + get, in in O(1) | Set size relative to input O(n) |

## Solution 1: Hashmap - Hashmap/Representation
```python
    def containsDuplicate(self, nums: List[int]) -> bool:

        # sc: hashmap relative to input O(n)
        count = defaultdict(int)

        # tc: iterate over list O(n)
        for num in nums:

            # tc: in operation constant O(1)
            if count[num] >= 1:
                return True
            # tc: put operation constant O(1)
            count[num] += 1

        # overall: tc  O(n) 
        # overall: tc O(n)
        return False
```
|  Aspect  | Time Complexity | Space Complexity |  Time Remarks | Space Remarks |
| -------- | --------------- | ---------------- | ------------- |  ------------ |
| Iteration | O(n) | O(n) | Iterate over array in linear O(n) + get, put in O(1) | Dictionary relative to input O(n) |
| Overall   | O(n) | O(n) | Iteration over input dominates, O(n)| Allocation for dictionary dominates, O(n) | 


## Solution 2: Set - Hashmap/Representation
```python
    def containsDuplicate(self, nums: List[int]) -> bool:
        
        # sc: set relative to input O(n)
        seen = set()

        # tc: iterate over list O(n)
        for n in nums:

            # tc: in operation O(1)
            if n in seen:
                return True
            # tc: add operation O(1)
            seen.add(n)
            
        # overall: tc O(n)
        # overall: sc O(n)
        return False
```
|  Aspect  | Time Complexity | Space Complexity |  Time Remarks | Space Remarks |
| -------- | --------------- | ---------------- | ------------- |  ------------ |
| Iteration | O(n) | O(n) | Iteration over list in linear O(n) + in, add operation O(1) | Set size relative to input O(n) |
| Overall | O(n) | O(n) | Iteration over list dominates, O(n) | Allocation relative to input dominates, O(n) | 


# 242. Valid Anagram ::2:: - Easy

Topics:  Hash Table, String, Sorting 

## Intro

> Given two strings s and t, return true if t is an 
> anagram of s, and false otherwise.
> An Anagram is a word or phrase formed by rearranging 
> the letters of a different word using all 
> original letters exactly once.

|  Example Input 'S'  | Example Input 'T' | Output |  
| ------------------- | ----------------- | ------ | 
| "anagram"           | "nagaram"         | true   |
| "rat"               | "car"             | false  |  

Constraints:

1 &le; s.length, t.length &le; 5 * 104

s and t consist of lowercase English letters

## Abstraction
Given two strings determine if they have the same 
character count.

## Space & Time Complexity
|  Solution  | Time Complexity | Space Complexity | Time Remark | Space Remark |  
| ---------- | --------------- | ---------------- | ----------- | ------------ |
| Hashmap | O(n) | O(1) | Two iterations over each word, linear O(n) + lookup, put in constant time O(1) | Dictionary will be proportional to input, space O(n) |
| Array of 26 | O(n) | O(1) | Single iteration over both words at same time O(n) + lookup, put in constant time O(1) | Array will be constant 26 elements, space O(1) |

## Solution 1: Hashmap Double Pass - Hashmap/Representation
```python
    def isAnagram(self, s: str, t: str) -> bool:
        
        # sc: hashmap of 26 elements O(1) 
        count = defaultdict(int)

        # tc: two iteration over string length O(n) * 2 ~= O(n)
        for x in s:
           count[x] += 1
        for x in t:
           count[x] -= 1

        # tc: iteration over char count O(n)
        for value in count.values():

            # tc: get() in constant O(1)
            if value != 0:
                return False

        # overall: tc O(n)
        # overall: sc O(1)
        return True
```
|  Aspect  | Time Complexity | Space Complexity |  Time Remarks | Space Remarks |
| -------- | --------------- | ---------------- | ------------- |  ------------ |
| Iteration | O(n) | O(n) | Iterate over two strings in linear O(n) + put, get in O(1) | Dictionary relative to input O(n) |
| Verification | O(n) | O(1) | Iterating over char count in linear O(n) + get in O(1) | No additional memory O(1) |
| Overall | O(n) | O(n) | Iteration over input dominates, O(n) | Allocation for dictionary relative to input dominates, O(n) |

## Solution 2: Array of 26 Single Pass - Hashmap/Representation
```python
    def isAnagram(self, s: str, t: str) -> bool:

        # Note:
        # ord() converts unicode char to int representation
        # and use int to index into array

        # sc: array of 26 constant O(1)
        count = [0] * 26

        # Check: mismatch length
        if len(s) != len(t):
            return False

        # tc: single iterate over both strings at same time O(n)
        for i in range(len(s)):
            count[ord(s[i]) - ord('a')] += 1
            count[ord(t[i]) - ord('a')] -= 1

        # tc: iterate over count array length 26 O(1)
        for val in count:
            if value != 0:
                return False

        # overall: tc O(n)
        # overall: sc O(1)
        return True
```
|  Aspect  | Time Complexity | Space Complexity |  Time Remarks | Space Remarks |
| -------- | --------------- | ---------------- | ------------- |  ------------ |
| Iteration | O(n) | O(1) | One iteration over both strings in linear O(n) + put, get in O(1) | Array of constant 26 size O(1) |
| Verification | O(n) | O(1) | Iterating over count array length 26 in constant O(1) | No additional memory O(1) |
| Overall | O(n) | O(1) | Iteration over input dominates, O(n) | Allocation for array is constant O(1) |


# 1. Two Sum I Not Sorted ::1:: - Easy

Topics:  Array, Hash Table 

## Intro
> Given an array of integers nums and an integer target, return 
> indices of two numbers such that they add up to target. You 
> can assume each test case only has one solution and 
> you may cant use the same element twice. Answer can
> be returned in any order.

|  nums[]         | Target    | Output |  
| --------------- | ----------| ------ | 
| [2,7,11,15]     | 9         | [0,1]  |
| [3,2,4]         | 6         | [1,2]  |  
| [3,3]           | 6         | [0,1]  |

Constraints:

Only one valid answer exists

2 &le; nums.length &le; 10^4

-10^9 &le; target &le; 10^9

## Abstraction
We need to find two elements that add up to the target
and return their indexes.

## Space & Time Complexity
|  Solution  | Time Complexity | Space Complexity | Time Remark | Space Remark |  
| ---------- | --------------- | ---------------- | ----------- | ------------ |
| Hashmap of Complements | O(n) | O(n) | Iterate over array O(n) | Allocation relative to input O(n) |

## Solution 1: Hashmap - Hashmap/Index with Data Key
```python
    def twoSum(self, nums: List[int], target: int) -> List[int]:
        
        # Note:
        # dictionary is complement -> index 

        # sc: dictionary size relative to input O(n)
        tracking = {}
 
        # tc: iterate over list O(n)
        for i in range (len(nums)):

            complement = target - nums[i]

            # tc: in operation O(1)
            if complement in tracking:
                return [i, tracking[complement]]
    
            # tc: put operation O(1)
            tracking[nums[i]] = i

        # overall: tc O(n) 
        # overall: sc O(n)
        return []
```
|  Aspect  | Time Complexity | Space Complexity |  Time Remarks | Space Remarks |
| -------- | --------------- | ---------------- | ------------- |  ------------ |
| Iteration | O(n) | O(n) | Iterate over list in linear O(n) + in, put O(n)  | Dictionary relative to input O(n) |
| Overall | O(n) | O(n) | Iterate over list dominates, O(n) | Allocation relative to input dominates, O(n) |


# 49. Group Anagrams ::2:: - Medium

Topics:  Array, Hash Table, String, Sorting

## Intro
> Given an array of strings strs, group the anagrams together. You 
> can return the answer in any order. An Anagram is a word formed by 
> rearranging the letters of a different word using all the original 
> letters exactly once.

|  Input                                   | Output                                      |  
| ---------------------------------------- | ------------------------------------------- | 
| ["eat","tea","tan","ate","nat","bat"]    | [["bat"],["nat","tan"],["ate","eat","tea"]] |
| [""]                                     | [[""]]
| ["a"]                                    | [["a"]]           

Constraints:

strs[i] consists of lowercase English letters

## Abstraction
Group the matching anagrams together.

## Space & Time Complexity
|  Solution  | Time Complexity | Space Complexity | Time Remark | Space Remark |  
| ---------- | --------------- | ---------------- | ----------- | ------------ |
| Array to Tuple Key  | O(n * k) | O(n * k) | Iterate over list O(n) * iterate over string O(m) | Dictionary relative to input O(n) |
| Array to String Key | O(n * k) | O(n * k) | Iterate over list O(n) * iterate over string O(m) | Dictionary relative to input O(n) |

## Bug: Mutable object, list, cannot be hashed
```python
    def groupAnagrams(self, strs: List[str]) -> List[List[str]]:
        
        anaGroup = {}
        
        for word in strs:
            
            charCount = [0] * 26
            for char in word:
                charCount[ord(char) - ord('a')] += 1
            
            # BUG: 
            # Attempted to hash mutable list charCount
            if charCount not in anaGroup:
                anaGroup[charCount] = []
            
            anaGroup[charCount].append(word)
        
        return list(anaGroup.values())
```

## Bug: Cannot use unsorted hashmap to tuple as key
```python
    def groupAnagrams(self, strs: List[str]) -> List[List[str]]:
        
        groupAna = {}

        for s in strs:
            count = defaultdict(int)

            for c in s:
                count[c] += 1

            tupKey = tuple(count)

            # BUG: 
            # hashmaps are not sorted
            # so tuple will vary (e.g., "cat" vs "tac")
            if tupKey not in groupAna:
                groupAna[tupKey] = []

            groupAna[tupKey].append(s)

        return list(groupAna.values())
```

## Solution 1: Array to Tuple Key - Hashmap/Grouping by Criteria
```python
    def groupAnagrams(self, strs: List[str]) -> List[List[str]]:
        
        # Note: 
        # Tuples immutable, allowing them to be hashed
        # Array allows for constant ordered representation

        # sc: relative to input unique anagram count m, O(m)

        # time complexity: iterating over all strings O(n) 
        for word in strs:
            
            # space complexity: fixed-sized array for 26 lowercase letters O(1)
            charCount = [0] * 26  
            
            # time complexity: counting chars in string of k length O(k)
            for char in word:
                charCount[ord(char) - ord('a')] += 1
            
            # space complexity: fixed-size tuple of length 26 O(1)
            key = tuple(charCount)  
            
            # time complexity: lookup operation of O(1) 
            if key not in anaGroup:
                anaGroup[key] = []  

            # time complexity: append operation to list O(1)
            anaGroup[key].append(word)  


        # overall: time complexity  O(n * k)  
        # overall: space complexity O(m * k)
        return list(anaGroup.values())
```

|  Aspect  | Time Complexity | Space Complexity |  Time Remarks | Space Remarks |
| -------- | --------------- | ---------------- | ------------- |  ------------ |
| Iterating over strings  | O(n) | O(1) | Loop over n strings O(n) | No additional memory per iteration O(1) |
| Counting Characters | O(k) per string | O(1) | counting characters for string k length O(k) | fixed-sized array for 26 chars O(1) |
| Tuple | O(1) | O(1) | Converting fixed-size array to tuple in constant O(1) | fixed-size tuple of 26 elements O(1) |
| Lookup in hashmap | O(1) | O(n * k) | Lookup operation takes constant O(1) | For n string keys of length k O(n * k)  |
| Appending to list | O(1) | O(n * k) | Appending operation takes constant O(1) | Hashmap of n keys to list of strings k length O(n * k) |
| Overall | O(n * k) | O(n * k) | Iteration and character counting dominates with O(n * k) | Memory allocation for n string keys with for list of strings k length dominates with O(n * k) |


## Solution 2: Array to String Key - Hashmap/Grouping by Criteria
```python
    def groupAnagrams(self, strs: List[str]) -> List[List[str]]:

        # Note: 
        # Appending to list then joining is more efficient than repeatedly 
        # appending to a string, it avoids the overhead of creating new string
        # objects on each append, overall: list in O(m), string in O(m^2)

        # sc: stores n tuple keys O(n) and lists of original k strings O(k), O(n * k)
        anaGroup = {}

        # tc: iterate over list O(n)
        for word in strs:

            # sc: array of 26 constant O(1)
            charCount = [0] * 26
            
            # tc: iterate over string O(m)
            for char in word:
                charCount[ord(char) - ord('a')] += 1
            
            # tc: array of 26 to list O(1)
            key_parts = []
            for count in charCount:
                key_parts.append(str(count))
                # delimiter
                key_parts.append("#")  

            # tc: concat list of 26 to string O(1)
            key = ''.join(key_parts)

            # tc: in operation O(1)
            if key not in anaGroup:
                # init empty list if no key doesn't exist
                anaGroup[key] = []

            # append word to anagram group
            anaGroup[key].append(word)

        # overall: tc
        # overall: sc 
        return list(anaGroup.values())
```

|  Aspect  | Time Complexity | Space Complexity |  Time Remarks | Space Remarks |
| -------- | --------------- | ---------------- | ------------- |  ------------ |
| Iterating over strings | O(n) | O(1) | Iterating over n strings O(n) | No additional memory per iteration O(1)  |
| Counting Characters | O(k) per string | O(1) | Counting characters for string k length | Fixed-sized character count array of 26 lowercase letters O(1) |
| Key Generator | O(1) | O(1) | Iterating over fixed-sized 26 character array to generate key O(1) | Unique string key generated O(1) |
| Lookup in hashmap | O(1) | O(n * k) | Lookup operation in hashmaps takes constant O(1) | Hashmap of n string keys to list of strings k length O(n * k)   |
| Appending to list | O(1) | O(1) | Appending operation to list takes constant O(1) | No additional memory required for append O(1) | 
| Overall | O(n * k) | O(n * k) | Iterating over n strings and counting characters for strings k length dominates O(n * k) | Hashmap of n string keys with lists of strings k length dominates O(n * k) |


# 347. Top K Elements in List ::3:: - Medium

Topics:  Array, Hash Table, Divide and Conquer, Sorting, Heap (Priority Queue), Bucket Sort, Counting, Quickselect

## Intro
> Given an integer array nums and an integer k, return the k most frequent element within the array. 
> Test cases are generated such that the answer is always unique. You may return the output in any order

|  Input             | k     | Output    |  
| -------------------| ----- | --------- | 
| [1,2,2,3,3,3,3]    | 2     | [2,3]     |
| [7,7]              | 1     | [7]       |

Constraints:

1 &le; k &le; number of distinct elements in nums

-1000 &le; nums[i] &le; 1000

## Abstraction
To find the k most frequent elements, we must first create an occurrence counter for each element in the list.
Now that we have the count, we just grab the top k highest occurring elements. 

## Space & Time Complexity
|  Solution  | Time Complexity | Space Complexity | Time Remark | Space Remark |  
| ---------- | --------------- | ---------------- | ----------- | ------------ |
| Brute Force (Insertion sort) | O(m<sup>2</sup>) | O(n + k) | Insertion sort on m unique elements dominates leadin | |
| Hashmap -> Bucket Sort | O(n) | O(n) | | |
| MinHeap |  |  |  |  |
| Divide and Conquer |  |  |  |  |
| Quickselect |  |  |  |  |

## Brute Force (Insertion sort)
```python
    def topKFrequent(self, nums: List[int], k: int) -> List[int]:
        
        # space complexity: frequency count for all unique integers O(m)
        count = defaultdict(int)

        # time complexity: iterate over list of n integers O(n)
        for num in nums: 

            # time complexity: insertion operation takes constant O(1)
            count[num] += 1


        # time complexity: iteration for conversion to list takes O(m)
        # space complexity: list size is equal to number of unique integers O(m)
        countTuple = list(count.items()) # [(num, freq), ...]

        # time complexity: Insertion sort 
        # Best case O(m), Average O(m) = O(m^2/2), Worst O(m^2)
        for i in range(1, len(countTuple)):
            key = countTup[i][1]
            j = i - 1

            # while left element is greater than key
            while j >= 0 and countTuple[j][1] < key:
                countTuple[j + 1] = countTuple[j]
                j -= 1
            
            countTuple[j + 1] = key # place key

        # time complexity: grab k top occurring ints from frequency list O(k)        
        result = []
        for i in range(k):
            
            # grabbing int key from tuple [(int, count), ...]
            result.append(countTuple[i][0]) 

        # overall: time complexity  O(m^2) = O(n + m^2 + k)
        # overall: space complexity O(m)
        return result
```
|  Aspect  | Time Complexity | Space Complexity |  Time Remarks | Space Remarks |
| -------- | --------------- | ---------------- | ------------- |  ------------ |
| Iteration + Insertion | O(n) | O(m) | Iterating over array of n integers O(n) | Hashmap with m unique elements results in O(m) memory allocation with O(n) in worst case |
| Conversion to list | O(m) | O(m) | Iterating over hashmap to create list takes O(m) | List of tuples stores m (int, count) pairs O(m) |
| Insertion sort | Best O(m), O(m<sup>2</sup>) average, O(m<sup>2</sup>) worst | Insertion sort on tuples of (int, count) sorting by count results in usual time complexity of insertion sort | No additional memory allocation required for in-place insertion sort O(1) |  |
| Grabbing top-k elements | O(k) | O(k) | Iterating over sorted list for k elements O(k) | Storing top k elements into result array O(k) | 
| Overall | O(m<sup>2</sup>) | O(m) | Insertion sort dominates leading to O(m<sup>2</sup>) | Hashmap and list for m unique elements dominates leading to O(m) |

## Find the Bug: Inverted Binary Search if statement:
```python
    def topKFrequent(self, nums: List[int], k: int) -> List[int]:
        
        def partitionSection(left, right, randIndex):
            freqPartition = frequency[unique[randIndex]]
            leftPartition = left
            unique[randIndex], unique[right] = unique[right], unique[randIndex]

            for i in range (left, right):
                if frequency[unique[i]] > freqPartition:
                    unique[i], unique[leftPartition] = unique[leftPartition], unique[i]
                    leftPartition += 1
            
            unique[right], unique[leftPartition] = unique[leftPartition], unique[right]

            return leftPartition

        def quickSelectBinarySearchHelper(left, right, finalMarker):
            
            # ran out of space
            if left == right:
                return

            randIndex = random.randint(left, right)
            resIndex = partitionSection(left, right, randIndex)

            if resIndex == finalMarker:
                return

            # INCORRECT:
            # this format is confusing
            # we are searching for the finalMarker, so always have that on the left
            # it should be, if the finalMarker is less than the result, then we shift right
            # current we are doing the reverse
            # should be:
            # elif finalMarker < resIndex 
            #   quickSelectBinarySearchHelper(left, resIndex-1, finalMarker)
            elif resIndex < finalMarker:
                quickSelectBinarySearchHelper(left, resIndex-1, finalMarker)
            else:
                quickSelectBinarySearchHelper(resIndex+1, right, finalMarker)

        # generate frequencies
        frequency = defaultdict(int)
        for n in nums:
            frequency[n] += 1
        
        # unique for keys
        unique = list(frequency.keys())
        n = len(unique)

        # start binarySearch
        l, r = 0, n-1

        # final index
        finalIndex = k-1


        quickSelectBinarySearchHelper(l, r, finalIndex)

        return unique[:k]
```


## Solution 1: MaxHeap Track N Elements - HashMap/Algorithm
```python    
    def topKFrequent(self, nums: List[int], k:int) -> List[int]:

        # Note: A heap only guarantees that the root is the max (max heap) or min (min heap).
        # The heap data structure only allows removing the root efficiently (O(log n))
        # The heap property says nothing about the relative order of leaves or internal nodes.
        # The smallest element could be anywhere in the leaves or internal nodes, not necessarily a leaf.
        # Thus for maxHeap, we need to add all of the elements to the tree
        # since we can't removed the smallest leaf as we go

        # space complexity: count for m unique elements worst case n elements O(n)
        count = defaultdict(int)

        # time complexity: iterate over list of n length O(n)
        for num in nums:
            count[num] += 1

        # space complexity: maxHeap must store all n elements O(n)
        maxHeap = []

        # time complexity: iteate over m unique elements worst case over n elements O(n)
        for num, freq in count.items():

            # time complexity: push onto heap O(log m) worst case O(log n) leading to best case O(m log m) worst case O(n log n) 
            heapq.heappush(maxHeap, (-freq, num))

        # space complexity: list of k elements worst case n elements best O(k) worst O(n)
        result = []

        # cannot iterate over array, as order is not guaranteed left to right
        # time complexity: iterate for k elements worst case n elements best O(k) worst O(n)
        for _ in range(k):

            # time complexity: pop top element O(log n) for k elements, worst case O(log n) for n elements, best case O(k log n) worst O(n log n)
            freq, num = heapq.heappop(maxHeap)
            result.append(num)

        # same as above
        # res = [num for _, num in (heapq.heappop(maxHeap) for _ in range(k))]

        # overall: time complexity O(n log n)
        # overall: space complexity O(n)
        return result
```
|  Aspect  | Time Complexity | Space Complexity |  Time Remarks | Space Remarks |
| -------- | --------------- | ---------------- | ------------- |  ------------ |
| Counting Frequencies | O(n) | O(n) | iterate trough input array of n length O(n) | Memory allocation for frequency count for n unique elements O(n) |
| Building MaxHeap | O(n log n) | O(n) | Push n elements O(n) using bubbleUp O(log n) leading to O(n log n) | Memory allocation for array representing heap O(n) |
| Extracting K elements | best: O(k log n) worst: O(n log n) | best: O(k) worst O(n) | Pop k elements O(k) using bubbleDown O(log n) leading to O(k log n) | Memory allocation to store k elements O(k) |
| overall | O(n log n) | O(n) | Heap construction dominates leading to O(n log n) | Memory allocation for hashmap and heap dominate leading to O(n) |


## Solution 2: MinHeap Track K Elements - HashMap/Algorithm
```python    
    def topKFrequent(self, nums: List[int], k:int) -> List[int]:

        # Note: A heap only guarantees that the root is the min (min heap) or max (max heap).
        # The heap data structure only allows removing the root efficiently (O(log n))
        # The heap property says nothing about the relative order of leaves or internal nodes.
        # The largest element could be anywhere in the leaves or internal nodes, not necessarily a leaf.
        # Thus for a minHeap, the root always holds the smallest element.
        # If we only remove the smallest element when heap as exceeded size k, 
        # the heap will always contains the k largest frequencies seen so far.

        # space complexity: count for m unique elements worst case n elements O(n)
        count = defaultdict(int)

        # time complexity: iterate over list of n length O(n)
        for num in nums: 
            count[num] += 1

        # space complexity: minHeap tracks k elements O(k)
        minHeap = []

        # time complexity: worst case iterate over n unique elements O(n)
        for num, freq in count.items(): 

            # time complexity: push onto heap O(log n) for n elements, leading to O(n log n) 
            heapq.heappush(minHeap, (freq, num)) 
            
            # if heap grows to size k + 1
            if len(minHeap) > k:

                # pop smallest element, heap will be back to size k
                # time complexity: # pop smallest O(log k) for worst case n elements, leading to O(n log k)
                heapq.heappop(minHeap) 
        
        # can iterate over heap array as only holding top k elements
        # time complexity: grab all k elements from minHeap worst case grab n elements O(n)
        result = [num for freq, num in minHeap]

        # for _, num in minHeap:
        #    result.append(num) 
        # same as above

        # overall: time complexity O(n log n)
        # overall: space complexity O(n)
        return result
```
|  Aspect  | Time Complexity | Space Complexity |  Time Remarks | Space Remarks |
| -------- | --------------- | ---------------- | ------------- |  ------------ |
| Counting Frequencies | O(n) | O(n) | iterate trough input array of n length O(n) | Memory allocation for frequency count for n unique elements O(n) |
| Building MinHeap | best: O(n log k) worst: O(n log n) | best: O(k) worst: O(n) | Iterates over list of n length O(n) and performs bubbleUp on heap with height k O(log k), leading to O(n log k) | Memory allocation to represent heap of k length O(k) |
| Extracting top k | best: O(k) worst: O(n) | best: O(k) worst: O(n) | Iterate over heap for k elements O(k) | Memory allocation to store top k elements O(k) |
| Overall | best: O(n log k) worst: O(n log n) | O(n) | Worst case iterating over count for n unique elements O(n) and pushing values to heap to track n values O(log n), leading to O(n log n) | Memory allocation to track in worst case n unique element counts O(n) |


## Solution 3: Descending QuickSelect BinarySearch - Hashmap/Algorithm
```python
    def topKFrequent(self, nums: List[int], k: int) -> List[int]:
    
        # Note: QuickSelect is a modified quicksort
        # Once the finalPartitionMarker is placed in its correct position, 
        # elements to the left and right are guaranteed to be smaller/larger
        # This avoids sorting the entire array and isolates the elements we need

        # In-place partition of array based on pivot value
        def partitionSection(left, right, randPivotElemIndex):
            
            # time complexity: partition processes m elements, n in worst case O(n)
            # Frequency of pivot element
            pivotElemFreq = frequency[unique[randPivotElemIndex]]

            # Move pivot to end
            # tuple unpacking in python, allows to swap two variables in a single line
            unique[randPivotElemIndex], unique[right] = unique[right], unique[randPivotElemIndex]
            
            # Index to partition larger elements to the left side of the section
            partitionIndex = left

            # Partition elements with freq more than pivotElemFreq to left
            # time complexity: iterates over unique elements between left and right segment O(m)
            for i in range(left, right):

                # Higher frequency -> "greater"
                if pivotElemFreq < frequency[unique[i]]:  

                    # swap larger element to left, sorting high to low
                    unique[i], unique[partitionIndex] = unique[partitionIndex], unique[i]
                    partitionIndex += 1

            # swap pivot to its final correct partitioned index
            # now elements to the right and left follow the conditions according to pivot element
            unique[partitionIndex], unique[right] = unique[right], unique[partitionIndex]
            return partitionIndex

        # time complexity: average recursion depth O(log m)
        # space complexity: recursion stack for in place partitioning on average O(log m)
        def quickSelectBinarySearchHelper(left, right, finalPartitionMarker):
            
            # Base Case: see 'For descending' below
            if left == right:
                return

            # Randomly choose a pivot index
            # differs from QuickSort "median-of-three" approach
            # random pivot is focused on finding the k-th smallest or largest
            # rather than fully sorting the array, random pivot avoids degrading to worst case O(n^2)
            randPivotElemIndex = random.randint(left, right)

            # Partition the array
            resultPivotElemIndex = partitionSection(left, right, randPivotElemIndex)
            
            # Base Case: see 'For descending' below
            if finalPartitionMarker == resultPivotElemIndex:
                return 
            
            # Binary Search Modification:
            # Selects next partition pivot
            # Recursively QuickSelect on partitioned section (left or right)
            # where finalPartitionMarker belongs to

            # finalPartitionMarker is in the left partition of resultPivotElemIndex
            elif finalPartitionMarker < resultPivotElemIndex:
                quickSelectBinarySearchHelper(left, resultPivotElemIndex - 1, finalPartitionMarker)
            
            # finalPartitionMarker is in the right partition of resultPivotElemIndex
            else:
                quickSelectBinarySearchHelper(resultPivotElemIndex + 1, right, finalPartitionMarker)

        # time complexity: iterate over list of n length O(n)
        frequency = defaultdict(int)
        for num in nums:
            frequency[num] += 1

        # QuickSelect BinarySearch to find the k most frequent elements
        unique = list(frequency.keys())
        n = len(unique)
        finalPartition = k - 1
        l, r = 0, n - 1

        # For descending:
        # say we have a list of 6 elements, we are trying to grab the largest 2 elements
        # if we do 2 - 1 we get 1.
        # so if we find we have partitioned and set index 1
        # we know that the elements 0 and 1 are the 2 largest elements
        # so we splice [:2] = [0, 1]
        quickSelectBinarySearchHelper(l, r, finalPartition)

        # overall: time complexity
        # overall: space complexity 
        return unique[:k]
```

## Solution 4: Ascending QuickSelect BinarySearch - Hashmap/Algorithm
```python
    def topKFrequent(self, nums: List[int], k: int) -> List[int]:

        # Note: QuickSelect is a modified quicksort
        # Once the finalPartitionMarker is placed in its correct position, 
        # elements to the left and right are guaranteed to be smaller/larger
        # This avoids sorting the entire array and isolates the elements we need

        # In-place partition of array based on pivot frequency (low to high sorting)
        def partitionSection(left, right, randPivotElemIndex):
            
            # Frequency of pivot element
            pivotElemFreq = frequency[unique[randPivotElemIndex]]

            # Move pivot to end
            # tuple unpacking in python, allows to swap two variables in a single line
            unique[randPivotElemIndex], unique[right] = unique[right], unique[randPivotElemIndex]
            
            # Index to place smaller elements
            partitionIndex = left

            # Partition elements with freq less than pivotElemFreq to left
            # time complexity: iterates over unique elements between left and right segment O(m)
            for i in range(left, right):

                # Lower frequency -> "lower"
                if frequency[unique[i]] < pivotElemFreq:

                    # swap larger element
                    unique[i], unique[partitionIndex] = unique[partitionIndex], unique[i]
                    partitionIndex += 1

            # swap pivot to its final correct partitioned index
            # now elements to the right and left follow the conditions according to pivot element
            unique[partitionIndex], unique[right] = unique[right], unique[partitionIndex]
            return partitionIndex

        # QuickSelect helper for low to high sorting
        # Avg recursion depth: O(log m), space: O(log m)
        def quickSelectBinarySearchHelper(left, right, finalPartitionMarker):
            
            # Base Case: see 'For ascending' below
            if left == right:
                return

            # Randomly choose a pivot index
            # differs from QuickSort "median-of-three" approach
            # random pivot is focused on finding the k-th smallest or largest
            # rather than fully sorting the array, random pivot avoids degrading to worst case O(n^2)
            partitionElemIndex = random.randint(left, right)
            
            # Partition the array
            finalPartitionElemIndex = partitionPivot(left, right, partitionElemIndex)

            # Base Case: see 'For ascending' below
            if finalPartitionMarker == finalPartitionElemIndex:
                return

            # Binary Search Modification:
            # Selects next partition pivot
            # Recursively QuickSelect on partitioned section (left or right)
            # where finalPartitionMarker belongs to

            # finalPartitionMarker is in the left partition of resultPivotElemIndex
            elif finalPartitionMarker < finalPartitionElemIndex:
                quickSelectBinarySearchHelper(left, finalPartitionElemIndex - 1, finalPartitionMarker)
            
            # finalPartitionMarker is in the right partition of resultPivotElemIndex
            else:
                quickSelectBinarySearchHelper(finalPartitionElemIndex + 1, right, finalPartitionMarker)

        # time complexity: iterate over list of n length O(n)        
        frequency = defaultdict(int)
        for num in nums:
            frequency[num] += 1

        # Quickselect to find the k most frequent elements
        unique = list(frequency.keys())
        n = len(unique)
        finalPartition = n - k
        l, r = 0, n - 1

        # For ascending:
        # say we have a list of 6 elements, we are trying to grab the largest 2 elements
        # if we do 6 - 2 we get 4.
        # so if we find we have partitioned and set index 4
        # we know that the elements 4 and 5 are the 2 largest elements
        # so we splice [6-2:] = [4:] = [4, 5]
        # quickSelectBinarySearchHelper(left, right, finalPartitionMarker)
        quickSelectBinarySearchHelper(l, r, finalPartition)

        # overall: time complexity
        # overall: space complexity
        return unique[n - k:]
```
|  Aspect  | Time Complexity | Space Complexity |  Time Remarks | Space Remarks |
| -------- | --------------- | ---------------- | ------------- |  ------------ |
| Frequency Count | O(n) | O(n) | Iterate over list of n length O(n) | Frequency count for at most n keys O(n) |
| Unique list | O(m) | O(m) | Grab unique keys from frequency map O(m) | Unique list contains m unique elements O(m) |
| Single Partition | O(m) | O(1) | Iterates over m unique elements in portion for a single partition O(m) | No additional memory allocation for in-place partition O(1) |
| Quickselect | O(m log m) | O(log m) | Recursion depth of O(log m) for O(m) elements per level O(m log m) | Recursion stack depth of O(log m) |
| Overall | O(n + m log m) | O(m) | Iterating over list of n length competes with Quickselect m number of unique elements O(m log m), depending if m &lt;&lt; n or m &#8776; n, leading to O(n + m log m) | Frequency count dominates leading to O(m) |

## Solution 5: BucketSort by Count - Hashmap/Algorithm
```python
    def topKFrequent(self, nums: List[int], k:int) -> List[int]:
            
        # space complexity: frequency count for unique integers O(m) 
        # time complexity: iterate over list of n integers O(n)
        count = defaultdict(int)
        for key in nums:

            # time complexity: insertion operation takes constant O(1)
            count[key] += 1

        # numBuckets equal to length to account for max possible frequency count
        # the case where list is full of only 1 element O(n)
        # + 1 for case of element with 1 list, need bucket of frequency 0 and bucket of frequency 1
        numBuckets = len(nums) + 1

        # below is saying: create a list of empty lists numBuckets amount of times 
        # time complexity: iterating to set empty list for each bucket
        # space complexity: creating O(n) bucket lists
        freqBuckets = [[] for i in range(numBuckets)]

        # time complexity: iterate over frequency list for m unique integer tuples (int, occurrences) O(m) 
        for int, occurrences in count.items():

            # time complexity: insert operation takes constant O(1)
            freqBuckets[occurrences].append(int)

        # space complexity: grabbing top k integers O(k)
        res = []

        # time complexity: iterate over n buckets O(n)
        for i in range(len(freqBuckets) - 1, 0, -1):
            
            # time complexity: iterate over all entries in current bucket O(n)
            for num in freqBuckets[i]:
                
                # time complexity: insert operation takes constant O(1)
                res.append(num)
                
                # time complexity: continue while less than k integers have been grabbed O(k)
                if len(res) == k:
                    return res

        # overall: time complexity O(n)
        # overall: space complexity O(n)
        return res
```

|  Aspect  | Time Complexity | Space Complexity |  Time Remarks | Space Remarks |
| -------- | --------------- | ---------------- | ------------- |  ------------ |
| Iteration + Insertion | O(n) | O(m) | Iteration over array of n integers O(n) | Hashmap for m unique integers O(m) |
| Bucket Initialization | O(n) | O(n) | Iterating over list length to create n empty buckets O(n) | At least n buckets needed for worst case scenario of list filled with only 1 integer O(n) |
| Bucket Population | O(m) | O(m) | Inserting m unique integers into their respective buckets |  Buckets will store m unique integers in their respective buckets O(m) |
| Result Extraction | O(n) | O(k) | Iterating over n buckets to collect k integers, worst case all buckets must be traverse O(n) | Top k integers must be stored in result list O(k) | 
| Overall | O(n) | O(n) | Iterating over n buckets dominates leading to O(n) | Initializing n buckets dominates leading to O(n) |



# 238. Product of Array Except Self ::2:: - Medium

Topics:  Array, Prefix Sum

## Intro
> Given an integer array nums, return an array answer such that answer[i] is 
> equal to the product of all the elements of nums except nums[i].
> The product of any prefix or suffix of nums is guaranteed to fit in a 32-bit integer.
> You must write an algorithm that runs in O(n) time and without using the division 
> operation.

|  Input         | Output          |  
| -------------- | --------------- | 
| [1,2,3,4]      | [24,12,8,6]     |
| [-1,1,0,-3,3]  | [0,0,9,0,0]     |

Constraints:

Product of any prefix or suffix is guaranteed to fit into 32 bit integer

## Abstraction
Given a list of nums, return a list of nums that is the product of the array 
excluding itself. For num n, the result should be the product of all the 
numbers to the left of it, times the product of all the numbers to the 
right of it.

## Space & Time Complexity
|  Solution  | Time Complexity | Space Complexity | Time Remark | Space Remark |  
| ---------- | --------------- | ---------------- | ----------- | ------------ |
| Brute Force (all pairs comparison) | O(n<sup>2</sup>) | O(n) | For each pair of elements, we compare them, leading to quadratic number of comparison O(n<sup>2</sup>) | Memory allocation for result array O(n) |
| Prefix & Postfix | O(n) | O(n) | Three iterations over the array for prefix, postfix, and result calculation O(n) = O(3n)  | Memory allocation for prefix, postfix, and result arrays O(n) = O(3n) |
| Prefix & Postfix | O(n) | O(1) | Two iterations over the array for prefix and postfix calculations O(n) |  |

## Brute Force (all pairs comparison)
```python
    def productExceptSelf(self, nums: List[int]) -> List[int]:

        # space complexity: result array of length n O(n)
        res = [0] * len(nums)

        # time complexity: iterate over list of n integers O(n) 
        for i in range(len(nums)):

            # starting value
            product = 1

            # time complexity: iterate over list of n integers O(n)
            for j in range(n):

                # skip current index i while multiplying
                if i != j:

                    # time complexity: multiplication operation takes constant O(1)
                    product *= nums[j]
                
            # time complexity: insert operation takes constant O(1)
            res[i] = product
        
        # overall: time complexity  O(n^2)
        # overall: space complexity O(n)
        return res
```

|  Aspect  | Time Complexity | Space Complexity |  Time Remarks | Space Remarks |
| -------- | --------------- | ---------------- | ------------- |  ------------ |
| Result array | O(1) | O(n) | Constant time to instantiate array of length n O(1) | Array to store n elements  O(n) |
| Outer loop | O(n) | O(1) | Iteration over the list takes linear time O(n) | No additional memory is allocated O(1) |
| Inner loop | O(n<sup>2</sup>) | O(1) | For each element in the outer loop, iterates through all remaining elements O(n<sup>2</sup>) | No additional memory is allocated O(1) |
| Overall | O(n<sup>2</sup>) | O(1) | The nested loops result in O(n<sup>2</sup>) time complexity. Derivation of average case is found O(n<sup>2</sup>) is found [here](/Notescommon-formulas#brute-case-double-for-loop). | Memory allocation for array to store n result elements O(n) |


## Solution 1:  Prefix and Postfix O(n) - Array/In Place Transformations
```python
    def productExceptSelf(self, nums: List[int]) -> List[int]:
        
        # space complexity: prefix, postfix, and result arrays to calculate and store final value for n integers O(n) = O(3n)
        prefix = [1] * len(nums)
        postfix = [1] * len(nums)
        res = [1] * len(nums)

        # Compute prefix products
        # time complexity: iteration over list of n length O(n)
        for i in range(1, len(nums)):
            
            # time complexity: lookup + multiplication operations take constant O(1)
            # (prefix of i) = (prefix of i - 1) * (nums[i - 1])
            prefix[i] = prefix[i - 1] * nums[i - 1]

        # Compute postfix products
        # time complexity: iteration over list of n length O(n)
        for i in range(n - 2, -1, -1):

            # time complexity: lookup + multiplication operations take constant O(1)
            # (postfix of i) = (postfix of i + 1) * (nums[i + 1])
            postfix[i] = postfix[i + 1] * nums[i + 1]

        # Combine prefix and postfix products
        # time complexity: iteration over list of length n O(n)
        for i in range(n):

            # time complexity: lookup + multiplication operations take constant O(1)
            # (product except self of i) = (prefix of i) * (postfix of i)
            res[i] = prefix[i] * postfix[i]


        # overall: time complexity O(n)
        # overall: space complexity O(n)
        return res
```

|  Aspect  | Time Complexity | Space Complexity |  Time Remarks | Space Remarks |
| -------- | --------------- | ---------------- | ------------- |  ------------ |
| Prefix Computation | O(n) | O(n) | Iterate over list of n length O(n) | Memory allocation for prefix array of n length O(n) |
| Postfix Computation | O(n) | O(n) | Iterate over list of n length O(n) | Memory allocation for postfix array of n length O(n) |
| Product except self computation | O(n) | O(n) | Iterate over prefix/postfix list of n length O(n) | Memory allocation for result array of n length O(n) |
| Overall | O(n) | O(n) | Iteration over list of n length takes linear time leading to O(n) | Memory allocation for list of n length leading to O(n) | 


## Solution 2: Prefix and Postfix Optimal O(1) - Array/In Place Transformations
```python
    def productExceptSelf(self, nums: List[int]) -> List[int]:
        
        # instantiate to 1, start prefix calculation at res[1], prefix of res[0] is always 1
        # space complexity: array to store results for n integers O(n) 
        res = [1] * len(nums)

        # Compute prefix products in res
        # time complexity: iterate over list of size n O(n)
        for i in range(1, len(nums)):

            # time complexity:
            # (prefix of i) = (prefix of i - 1) * (num at i - 1)
            res[i] = res[i - 1] * nums[i - 1]

        # accumulate running postfix through reverse iteration
        # postfix starts at 1, the postfix of len(n) - 1
        postfix = 1

        # start iteration at (len(n)-1), postfix of last integer will always be 1
        # time complexity: iterate over list of size n in reverse O(n)
        for i in range(len(nums) - 1, -1, -1):
            
            # (product except i) = (prefix of i) * (postfix of i)
            res[i] *= postfix
            
            # (postfix of i - 1) = (postfix of i) * (num at i) 
            postfix *= nums[i]

        # overall: time complexity O(n)
        # overall: space complexity O(1)
        return res
```

|  Aspect  | Time Complexity | Space Complexity |  Time Remarks | Space Remarks |
| -------- | --------------- | ---------------- | ------------- |  ------------ |
| Prefix Computation | O(n) | O(1) | Iteration over list of n length O(n) | Prefix value stored directly in result array O(1) |
| Postfix Computation + Product except self computation | Iteration over list of n length O(n) | Postfix value stored in integer, product except self stored in result array O(1) |  |  |
| Overall | O(n) | O(1) | Iteration over list of n length O(n) | Prefix, postfix, and product except self all stored in result array or an integer leading to constant O(1) |


# 36. Valid Sudoku ::2:: - Medium

Topics:  Array, Hash Table, Matrix

## Intro
> Determine if a 9 x 9 Sudoku board is valid. Only the filled cells need to be validated according to the following rules:
> Each row must contain the digits 1-9 without repetition.
> Each column must contain the digits 1-9 without repetition.
> Each of the nine 3 x 3 sub-boxes of the grid must contain the digits 1-9 without repetition.

Constraints:

Only the currently filled cells need to be validated, regardless is sudoku board
is actually solvable or not.

|  Input         | Output          |  
| -------------- | --------------- | 
| a sudoku table is too big to put here lol | just look at -> [sudoku board example](https://leetcode.com/problems/valid-sudoku/description/) | 

## Abstraction
Abstract board into sets of rows, cols, and boxes, and validate whether 
duplicate values exist in any set.

## Space & Time Complexity
|  Solution  | Time Complexity | Space Complexity | Time Remark | Space Remark |  
| ---------- | --------------- | ---------------- | ----------- | ------------ |
| Brute Force | O(n<sup>2</sup>) = O(3 * n<sup>2</sup>)  |  |  | |
| Hashmap | O(n<sup>2</sup>) | O(n) |  |  |

## Brute Force (check row, column, box)
```python
    def isValidSudoku(self, board: List[List[str]]) -> bool:
        
        # time complexity: iterating over row, col, or box (flattened) of n length (+ O(n)) not (* O(n))
        def isValidLine(line: List[str]) -> bool:
            seen = set()
            for num in line:
                if num != ".":
                    if num in seen:
                        return False
                    seen.add(num)
            return True

        # time complexity: iterating over board with n rows of length n and validating O(n^2)
        for row in board:
            if not isValidLine(row):
                return False

        # time complexity: iterating over board with n columns of length n and validating O(n^2)
        for c in range(9):
            column = []
            for r in range(9):
                column.append(board[r][c])
            if not isValidLine(column):
                return False

        # time complexity: iterating over n boxes of flattened length n <- (n x n) and validating O(n^2)
        for box_row in range(0, 9, 3):
            for box_col in range(0, 9, 3):
                line = []
                for r in range(box_row, box_row + 3):
                    for c in range(box_col, box_col + 3):
                        line.append(board[r][c])

                if not isValidLine(line):
                    return False

        # overall: time complexity O(n^2) = (3 * n^2)
        # overall: space complexity O(1)
        return True
```

## Find the bug: Creating defaultdict of sets incorrectly
```python
    def isValidSudoku(self, board: List[List[str]]) -> bool:

        # INCORRECT:
        # first argument must be callable or None
        # should be: defaultdict(set)
        # [] vs list vs set
        # [] is an instance of the list
        # list is the class that creates a new list object when called
        # set is the class that creates a new list object when called
        rows = defaultdict([])
        cols = defaultdict([])
        boxs = defaultdict([])

        for r in range(9):
            for c in range(9):
                tmp = board[r][c]

                if tmp != ".":

                    boxKey = (r//3, c//3)

                    if (tmp in rows[r] or
                        tmp in cols[c] or
                        tmp in boxs[boxKey]):
                        return False

                    rows[r].add(tmp)
                    cols[c].add(tmp)
                    boxs[boxKey].add(tmp)
        return True
```

## Find the bug: Did not index into 9 boxes correctly
```python
    def isValidSudoku(self, board: List[List[str]]) -> bool:
        rows = defaultdict(set)
        cols = defaultdict(set)
        boxs = defaultdict(set)


        for r in range(9):
            for c in range(9):
                tmp = board[r][c]

                if tmp != ".":

                    # INCORRECT:
                    # this will create 81 possible boxes
                    # since 9 possible rows and columns
                    # 9 possible, 9 possible = 81 possibilities
                    # should have been:
                    # boxKey = tuple((r//3, c//3))
                    # we want 9 boxes
                    # (9//3), (9//3) = 
                    # 3 possible, 3 possible = 9 possibilities
                    boxKey = tuple((r, c))

                    if (tmp in rows[r] or
                        tmp in cols[c] or
                        tmp in boxs[boxKey]):
                        return False

                    rows[r].add(tmp)
                    cols[c].add(tmp)
                    boxs[boxKey].add(tmp)

        return True
```

## Solution 1: Defaultdict Matrix - Hashmap/Representation
```python 
    def isValidSudoku(self, board: List[List[str]]) -> bool:
        
        # space complexity: hashsets for rows, columns, and flattened boxes of n length O(n) = O(3n)
        cols = defaultdict(set)
        rows = defaultdict(set)
        grids = defaultdict(set)

        # time complexity: iterating over all cells (r * c) O(n^2)
        for r in range(9):
            for c in range(9):
                
                tmp = board[r][c]
                if tmp != ".":

                    # indexing box sets by tuple key (r/3, c/3)
                    gridTuple = (r // 3, c // 3)

                    # time complexity: lookup operation for element to corresponding sets in constant O(1)
                    if (tmp in rows[r] or 
                        tmp in cols[c] or 
                        tmp in grids[gridTuple] ):
                        return False
                    
                    # time complexity: insert operation for new element to corresponding sets in constant O(1)
                    cols[c].add(tmp)
                    rows[r].add(tmp)
                    grids[gridTuple].add(tmp)

        # overall: time complexity O(n^2)
        # overall: space complexity O(n)
        return True
```

## Solution 2: [[]] Matrix - Hashmap/Representation
```python
    def isValidSudoku(self, board: List[List[str]]) -> bool:
        
        # space complexity: hashsets for rows, columns, and flattened boxes of n length O(n) = O(3n)
        rows = [[], [], [], [], [], [], [], [], []]
        col = [[], [], [], [], [], [], [], [], []]
        grids = [[], [], [], [], [], [], [], [], []]

        # time complexity: iterating over all cells (r * c) O(n^2)
        for r in range(9):
            for c in range(9):
                
                tmp = board[r][c]
                if tmp != ".":  

                    # indexing box sets by unique int calculation 
                    #  (j//3) = 0, 1, 2
                    #  (3 * (i//3)) = 0, 3, 6
                    #    0 1 2
                    # 0  0 1 2
                    # 3  3 4 5
                    # 6  6 7 8
                    # leads to 0-8 index indexing
                    gridKey = (3 * (i//3)) + (j//3)

                    # time complexity: lookup operation for element to corresponding sets in constant O(1)
                    if (tmp in rows[r] or 
                        tmp in col[c]  or 
                        tmp in grids[gridKey]):
                        return False

                    # space complexity: lookup operation for new element to corresponding sets in constant O(1)
                    col[c].append(tmp)
                    rows[r].append(tmp)
                    grids[gridKey].append(tmp)
        
        # overall: time complexity O(n^2)
        # overall: space complexity  O(n)
        return True
```

|  Aspect  | Time Complexity | Space Complexity |  Time Remarks | Space Remarks |
| -------- | --------------- | ---------------- | ------------- |  ------------ |
| Row, Col, Box Initialization | O(1) | O(n<sup>2</sup>) |  | Memory allocation for n rows, cols, boxs, of n length O(n<sup>2</sup>) |
| Iterating over all cells | O(n<sup>2</sup>) | O(1) | Iterating over n x n board O(n<sup>2</sup>) | No additional memory allocation for iteration O(1) |
| Lookup | O(1) | O(1) | Lookup operation takes constant O(1) | No additional memory allocation for lookups O(1)  |
| Insertion | O(1) | O(n<sup>2</sup>) | Insertion operation takes constant O(1) | List of n lists each of n length O(n<sup>2</sup>) | 
| Overall | O(n<sup>2</sup>) | O(n<sup>2</sup>) | Iterating over all cells in n x n board dominates, leading to O(n<sup>2</sup>) | List of n lists of n length dominates leading to O(n<sup>2</sup>)  |

Note: Solution 1.2 is faster than Solution 1:

While sets are efficient for large data due to constant time insert, lookup, delete, 
for a fixed-sized 9x9 board like this, the overhead of defaultdict and 
set operations is slower than the [[]] solution. 

# 128. Longest Consecutive Sequence ::3:: - Medium

Topics:  Array, Hash Table, Union Find

## Intro
> Given an array of integers nums, return the length of the longest consecutive sequence of elements.
> A consecutive sequence is a sequence of elements in which each element is exactly 1 greater than 
> the previous element
> You must wrtie an algorithm that runs in O(n) time.

|  Input                    | Output  |  
| ------------------------- | ------- | 
| [100,4,200,1,3,2]   | 4 from [1, 2, 3, 4]      | 
| [0,3,7,2,5,8,4,6,0,1]  | 9 from [0, 1, 2, 3, 4, 5, 6, 7, 8]  |
| [1, 0, 1, 2] | 3 from [0, 1, 2] |

Constraints:

0 &leq; nums.length &leq; 10<sup>5</sup>

-10<sup>9</sup> &leq; nums[i] &leq; 10<sup>9</sup>

## Abstraction
Give a list of integers, find the longest sequence of increasing integers.
This sequence does not have to follow the left to right order of the array.

## Space & Time Complexity
|  Solution  | Time Complexity | Space Complexity | Time Remark | Space Remark |  
| ---------- | --------------- | ---------------- | ----------- | ------------ |
| Brute Force |  |  |  | |
| Hashmap |  |  |  |  |

## Brute Force
```python
    def longestConsecutive(nums: List[int]) -> int:
        
        longest = 0

        # time complexity: Iterate over list of n length O(n)
        for num in nums:

            currLen = 1
            
            # time complexity: Iterate over potential n consecutive integers O(n)
            # time complexity: Lookup in list takes O(n)
            # time complexity: O(n^2)
            while num + currLen in nums:
                currLen += 1

            # compare to global max
            longest = max(longest, currLen)

        # Overall time complexity: O(n^3) for worst-case where we scan sequences n times for n numbers
        # Space complexity: O(1)
        return longest
```
|  Aspect  | Time Complexity | Space Complexity | Time Remark | Space Remark |  
| ---------- | --------------- | ---------------- | ----------- | ------------ |
| Iterate | O(n) | O(1) | Iterate over list of n length O(n) | No additional memory allocation for iteration O(1) |
| Iterate consecutive sequence | O(n) |O(1) | Iterate over potential n consecutive integers O(n) | No additional memory allocation for iteration O(1) |
| List lookup | O(n) | O(1) | Lookup in list takes O(n) | No additional memory allocation for list lookup O(1) |
| Overall | O(n<sup>3</sup>) | O(1) | Iterating * Iterating * Lookup dominate leading to O(n<sup>3</sup>) | No additional memory allocation O(1) |

## Find the Bug: Empty list for Union tree leads to fail
```python
    def longestConsecutive(self, nums: List[int]) -> int:

        # INCORRECT:
        # need to account for empty list case
        # or else max(size.values()) will return error
        # should be:
        # if len(nums) == 0
        #   return 0

        parent = {}
        size = {}

        def find(x):

            if parent[x] != x:
                parent[x] = find(parent[x])

            return parent[x]


        def union(x, y):
            root_x = find(x)
            root_y = find(y)

            if root_x != root_y:

                size_x = size[root_x]
                size_y = size[root_y]

                if size_x < size_y:
                    parent[root_x] = root_y
                    size[root_y] += size[root_x]
                else:
                    parent[root_y] = root_x
                    size[root_x] += size[root_y]

        setNum = set(nums)
        for n in setNum:
            parent[n] = n
            size[n] = 1

        for n in setNum:
            if (n+1) in parent:
                union(n, n+1)

        return max(size.values())
        
```

## Solution 1: HashMap Continent Boundaries - HashMap/Representation
```python
    def longestConsecutive(nums: List[int]) -> int:
        
        # space complexity: hashmap of list of n length O(n)
        seqLen = defaultdict(int)
        longest = 0

        # Handle duplicates
        nums = set(nums)  

        # Iterate over unique numbers
        for num in nums:
            # Skip if num is already part of an existing sequence
            # Fetch lengths of neighboring sequences
            leftContinentLenFromBoundary = seqLen[num - 1]
            rightContinentLenFromBoundary = seqLen[num + 1]

            # Calculate new sequence length
            bridgedLen = 1 + leftContinentLenFromBoundary + rightContinentLenFromBoundary

            # Update new continent boundaries
            seqLen[num - leftContinentLenFromBoundary] = bridgedLen
            seqLen[num + rightContinentLenFromBoundary] = bridgedLen

            # Update the global max
            longest = max(longest, bridgedLen)

        return longest
```
|  Aspect  | Time Complexity | Space Complexity | Time Remark | Space Remark |  
| ---------- | --------------- | ---------------- | ----------- | ------------ |
| Iteration + Insertion | O(n) | O(n) | Iterating over list of n length O(n) | Memory allocation for sequence length for n integers O(n) |
| Lookups | O(1) | O(1) | Lookup operation takes constant O(1) | No additional memory allocation needed for lookup O(1) |
| Updating sequence count | O(1) | O(1) | Boundary updates for left most, curr, and right most in constant O(1) | Insert operation takes constant O(1) | 
| Overall | O(n) | O(n) | Iterating over list of n length dominates O(n) | Memory allocation for n sequences O(n) | 

## Solution 2: Set Rummy Run - Hashmap/Representation
```python
def longestConsecutive(self, nums: List[int]) -> int: 
    
    # space complexity: set for list of n length O(n)
    numSet = set(nums)
    longest = 0

    # time complexity: iterate over list of n length O(n)
    for num in numSet:

        # found start of a run
        # time complexity: lookup operation takes constant O(1)
        if (num - 1) not in numSet: 

            # rummy run until missing an element
            currLen = 1
            # time complexity: iterate over m sequence worst case n sequence O(n), lookup O(1), leading to O(n)
            while (num + currLen) in numSet:
                currLen += 1 

            # validate with global max sequence
            longest = max(longest, currLen)

    # overall: time complexity O(n)
    # overall: space complexity O(n)
    return longest        
```

|  Aspect  | Time Complexity | Space Complexity |  Time Remarks | Space Remarks |
| -------- | --------------- | ---------------- | ------------- |  ------------ |
| Hashset | O(n) | O(n) | Converting list to set uses insert operation O(1) for n linear O(n) | Memory allocation for list of n length O(n) |
| Outer Loop | O(n) | O(1) | Iterating over set of n length O(n) | No additional memory allocation for iteration |
| Start of sequence check | O(1) | O(1) | Lookup operation takes constant O(1) | No additional space of lookup operation O(1) |
| while loop sequence | O(n) | O(1) | Iterate white sequence is valid, note all numbers are checked only once due to start of sequence check O(n) | No additional memory allocation for while loop O(1) |
| Overall | O(n) | O(n) | Iterating over hashset of n length dominates leading to O(n) | Memory allocation for hashset of list n length dominates o(n) | 



## Solution 3: Union Find Tree - HashMap/Algorithm
```python
    def longestConsecutive(self, nums: List[int]) -> int:
        
        if len(nums) == 0:
            return 0

        # Union-Find Initialization:
        
        # Stores parent of each number
        # space complexity: hashmap of size n O(n)
        parent = {}  

        # Stores the size of connected components
        # space complexity: hashmap of size n O(n)
        size = {}    

        # Find operation with path compression
        # time complexity: Amortized O((n)) per operation
        def find(x):

            # if the parent of x is not itself, we have not reached the representative  
            if parent[x] != x:

                # Path compression
                # set parent of curr, to parent of parent
                parent[x] = find(parent[x])

            # return representative of group
            # either return self
            # or return parent of parent
            return parent[x]

        # Union operation with size optimization
        # time complexity: Amortized O((n)) per operation
        def union(x, y):

            # grab representatives of both x and y
            rootX = find(x)
            rootY = find(y)

            # trees are not already connected
            if rootX != rootY:

                # grab tree sizes
                xSize = size[rootX]
                ySize = size[rootY]

                # x tree is larger
                if ySize < xSize:
                    # attach smaller tree to larger tree
                    parent[rootY] = rootX
                    # add smaller tree size to larger tree
                    size[rootX] += size[rootY]

                # y tree is larger
                else:
                    # attach smaller tree to larger tree
                    parent[rootX] = rootY
                    # add smaller tree size to larger tree
                    size[rootY] += size[rootX]

        # Initialize Union-Find structure: 
        # Set all parents to self
        # time complexity: iterate over list of n length O(n)
        for num in nums:
            if num not in parent:
                parent[num] = num
                size[num] = 1

        # Join consecutive numbers: 
        # Union operation on sequences
        # time complexity: iterate over list of n length O(n)
        for num in nums:
            if num + 1 in parent:
                union(num, num + 1)
        
        # return largest group

        # overall: time complexity O(n)
        # overall: space complexity O(n)
        return max(size.values())
```
|  Aspect  | Time Complexity | Space Complexity |  Time Remarks | Space Remarks |
| -------- | --------------- | ---------------- | ------------- |  ------------ |
| Initialization | O(n) | O(n) | Insert n elements into parent and size hashmaps O(n) | Memory allocation for parent and size for n elements O(n) |
| Union-Find Operations | Amortized O((n)) | O(1) | (n) is nearly constant for realistic input sizes | No additional memory allocation for Union-Find operations O(1) |
| Join Operation | O(n) | O(1) |  |  |
| Grab Largest | O(n) | O(1) | Grab max size in list of n elements O(n) | No additional memory allocation for grab |
| Overall | O(n) | O(n) | Initialization dominates, leading to O(n) |  | 
