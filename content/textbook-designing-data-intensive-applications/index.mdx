---
title: "Textbook: Designing Data Intensive Applications"
description: "designing data intensive applications"
image: "../../public/Notes/ddia-2.png"
publishedAt: "2025-09-18"
updatedAt: "2025-09-18"
author: "jonathancamberos"
isPublished: true
tags:
- system design
---


# Preface

> Hello! all of these notes will be in short hand form, to avoid rewriting the textbook from scratch.
> Additionally, I am reading/writing this in my vscode, i know i know, editing text in vscode,
> an editor for code, WITH CODE IN THE NAME, disgraceful, buttt the rest of my LeetCode notes
> are here so ¯\\_(ツ)_/¯.
> As such, the notes look much prettier (and colored) in vscode, if you want to read along, 
> plz check my github! - jonathan

> Additionally, if this looks bad in full screen, that is because I
> only read the notes in 2/3rds split screen and I dont have the need 
> to fix it ¯\\_(ツ)_/¯.

## Intro

Backend swe's have new common buzz words relating
to the storage and processing of data.

NoSQL! BIG DATA! Web scale, Sharing, EvEnTuAl CoNsIsTeNcY, ACID!!! (no not the drug)
The Cap Theorem, Clooooud services, (you get the point).

Data intensive applications are pushing the boundaries of what is possible
by inventing new buzzwords, strategies, and technologies.

Fortunately, regardless of the rapid change
basic principles remain true no matter what tool or version you are using.

We are here to learn the 'why' of those basic principles.

The goal of these notes is to organize shorthand to learn these concepts.

## Who Should Read This Book?

> Fellow developers, old friends, random strangers on the internet who
> have stumbled upon my chicken scratch of notes. Welcome all!

But again, if you are backend these notes are for you.

1. **Role wise, this is for** 

- **software engineers**: the builders

- **software architects**: the designers

- basically **anybody** making decisions on the 
architecture of the systems you are building.

- for people with natural curiosity for what is going on under the hood,
rule number 1: its never magic (thanks bobby ;) )
rule number 2: i will not bail you out of jail (also thank you bobby, rip cmsc417 gang)

2. **App wise, this is for** 

- **scalable data systems**: (e.g., supporting web or mobile apps with millions of users)

- **minimizing downtime**: (e.g., making apps available)

- **maintainable systems**: (e.g., making systems easier scale and update as technologies change)

3. **Counter Argument wise, this is for**

- when someone complains "you're not google or amazon, stop worrying about scale
and just use a relational database". You can **slap them with a fish and say**
"fair, while building for scale you don't need is wasted effort, it may lock you
into an inflexible design. You should know its important to choose the right tool 
for the job and while relational databases are important, they are not the final word
on dealing with data". They will then view you as a worthy advisory and challenge
you to a fight to the death (dont forget what you learned in your 4 years of computer science
undergrad and give them the classic fork() and os.kill_child() leading to instant victory, for
legal reasons this is a joke please dont stop reading its like 4 in the morning im sorry,
also if you laughed at that youre a nerd :) )


## Scope Of This Book

> Again, the *architecture* of data systems.

We cover the principles and tradeoffs to data systems 
and explore design decisions taken by different products.

We leave deployment, operations, security, management, and other areas,
to be covered by other books.

## Outline Of This Book

> 1. Part I: The Basics: our bearings, and definitions

We define the fundamental ideas for the design of data intensive applications.

|  Chapter  | Topic |
| -------- | --------------- |
| 1 | Top down point of view. What are we trying to achieve? Reliability, scalability, maintainability. |
| 2 | Start by comparing different data models and query languages, and see how they are appropriate to different situations |
| 3 | Storage engines. How databases arrange data on disk so that we can find it again efficiently |
| 4 | Formats for data encoding (serialization) and evolution of schemas over time |



> 2. Part II: From a single machine to a distributed system

Shift from single machine to distributed across multiple machines, 
as often necessary for scalability, and discuss variety of unique challenges

|  Chapter  | Topic |
| -------- | --------------- |
| 5 | Replication |
| 6 | Partitioning and sharding |
| 7 | Transactions |
| 8 | Dive into more details on the problems prevalent with distributed systems |
| 9 | What is means to achieve consistency and consensus in a distributed system |

> 3. Part III: Heterogeneous systems, where no one data base can do it all

Learn about systems that derive some datasets from other datasets.
in heterogenous systems, when no one database can do everything well.
Applications need to connect different databases, caches, indexes and so on.

|  Chapter  | Topic |
| -------- | --------------- |
| 10 | Batch processing approach to derived data. |
| 11 | Build upon batch processing with stream processing. |
| 12 | Tape everything together and discuss approaches for building reliable, scalable, and maintainable applications in the future. |

## References and Further Reading

> Nothing new here!

Most of everything in this book has been said before!
Free references and links as we go along.


# Part I: Foundations of Data Systems

## Intro

> The first four chapters go through the fundamental ideas that apply to all data systems,
> whether running on a single machine or distributed across a cluster of machines.

1. **Chapter 1:**

- Define terminology and approach we will use.
- Define words like reliability, scalability, and maintainability
- Explain how we try to achieve these goals.

2. **Chapter 2:**

- Compare data models and query languages.
- See how different models are fit different situations.

3. **Chapter 3:**

- Look at internals of storage engines.
- Look at how database put data on disk.
- Different storage engines are optimized for different workloads.
- Choosing the right one can have a huge effect on performance.

4. Chapter 4: 

- Compare formats of data encoding/serialization
- Examine in an environment where application requirements change and schemas need to adapt over time.

## Reliable, Scalable, and Maintainable Applications

> Many applications today are data intensive, as opposed to compute intensive.
> Raw CPU power is rarely a limiting factor vs the amount of data, complexity of data, 
> and speed at which it is changing.

Data intensive applications are typically built from standard building blocks for 
functionality, not limited to but including:

- **Databases**: Storing data, for other apps or later use 

- **Caches**: Remember result of expensive operations, to speed up reads

- **Search Indexes**: Allowing users to search data by keywords or filter in various ways

- **Batch Process**: Periodically crunching a large amount of accumulated data

Again, obvious yes, but thats the point!

These data systems are such a **successful abstraction** that most **sane** engineers wouldn't 
think of writing a new data storage engine from scratch because perfectly good ones already exists.

But again, right tool for the right job.
While there are many database systems, each have different characteristics
as different applications have different requirements.

## Thinking About Data Systems

> Why do we lump databases, queues, caches, etc, each with different patterns, performance, 
> and implementations under the umbrella term data systems?
> Why am I starting each section with a quote?
> Do I have voices in my head?!? Maybe...
> But still, the umbrella makes it confusing!

New tools for data storage and processing emerged leading to trouble
as traditional categories fad and boundaries blur.

1. **Redis**: both a datastore and message queue
2. **Apache Kafka**: a message queue with database durability

No single tool can meet all data processing and storage needs as applications
have such demanding and wide ranging requirements.

As no single tool can meet all data processing and storage needs, work is broken down
into tasks, that can be performed on a single tool, which itself is using multiple tools.

> Take an simple API

An API usually hides implementation details from clients, and thus, becomes a 
special purpose data system from smaller, general purpose components.

We could even add guarantees such as cache will be invalidated and updated on writes so
clients see consistent results, which leads wrappers to become responsible for
both data storage and system design.

But now even more questions arise! How do you provide good performance?
How do we handle increase in load? etc. Lets put that in our back pocket (ignore) 
for now and continue.

Lets jump into our main sections, reliability, scalability, and maintainability.

## Reliability

> The system should continue to work correctly (performing the correct function at the
> desired level of performance) even in the face of *gasp adversity*, either 
> hardware or software faults, human error, etc..

## Intro

Typical expectations for software to be reliable include:

- Application performs function user expected
- Tolerates user making mistakes or using software in unexpected ways
- Good enough performance for required use case under expected load and volume
- System prevents unauthorized access and abuse

> A simple rule of thumb for reliability is 'continuing to work correctly, even
> when things go wrong.'

A fault, is the things that can go wrong
A system that anticipate faults is thus fault-tolerant or resilient,
tolerating certain types of faults.

A failure is when the system as a whole stops providing the required service
to the user.

It is nearly impossible to prevent all type of faults, which is why we prefer 
fault-tolerant systems.

## Hardware Faults

> Common when discussing system failures

- Hard disk crash
- Faulty RAM
- Power grid blackout
- Unplugging the wrong network cable.

A good rule of thumb to explain how common it is:
the MTTF (mean to time failure) of a hard disk is about 10 to 15 years.
Thus, in a storage cluster with 10,000 disks around one disk dies per day.

## Software Errors

Systematic errors can occur within the system, and such faults are harder to
anticipate because they are correlated 'non-random' across nodes, and tend to 
cause more system failures than uncorrelated 'random, ex: MTTF' hardware faults.

- Software bug that causes app instance to crash on particular bad input
- Runaway process that uses up shared resources, CPU, memory, disk, bandwidth
- Services slows down which other system depends on
- Cascading failures, small fault triggers fault in another component

These faults lie dormant for a long time until they are triggered by an unusual
set of circumstances.


## Human Errors

Humans are known to be unreliable, even with the best intentions.

A good rule of thumb to explain how common it is:
Study of large internet services showed configuration errors by operators 
were the leading causes of outages, where hardware faults were only 10-25%

How do we make system reliable in spite of humans?

- Design systems with minimal opportunity for error, for example by 
well designed abstraction, APIs, and admin interfaces.
- Decouple where people make the most mistakes from places where they can cause errors
- Test thoroughly on all levels, from unit tests to whole system integration tests
- Allow quick and easy recovery from human errors to minimize impact
- Telemetry for performance metrics and error rates 

## How Important is Reliability?

Reliability is not just for power stations and air traffic control.
More mundane applications such as photo applications for parents as an example.

There are situations in which we can sacrifice reliability in order to reduce 
development cost, but we should always be conscious of cutting corners.

## Scalability

> As the system grows (in data volume, traffic volume, or complexity), there should 
> be a reasonable solution to deal with that growth

If a system is reliable today, that is not a guarantee that it will work reliably 
in the future.
A common reason for degradation is increased load. A jump from 10,000 concurrent
to 100,000 or 1 million to 10 million will require processing much larger volumes
of data than before

> Scalability is then, a system's ability to cope with increased load. 
> Not one dimensionally, but more so, 'if the system grows in a particular way,
what are our options for coping with the growth' or 'how can we add computing 
resources to handle the additional load'

## Describing Load Via Twitter (yes twitter not 'X')

> Load can be described with a few numbers which we call load parameters.
> The best choice of parameters depends on the architecture of your system.

Examples of load parameters could be:

- Requests per second to a web server.
- A ratio of reads to writes in a database.
- The number of simultaneously active users in a chat room
- Hit rate on a cache
- The average case or a bottleneck of some specific case

> Twitter Scalability (ehm sorry 'X') Post/Home

Published in Nov 2012, Twitters two main operations are

1. Post Tweet
A user can publish a new message to their followers. 4.6k requests/sec on average,
over 12k request/sec at peak

2. Home timeline
A user can view tweets posted by the people they follow. 300k requests/sec

Handling 12,000 writes per second is simple. The issue lies not with tweet volume 
but with fan out.

Each user follows many people, and each user is followed by many people.
Thus broadly, there are two ways of implementing these two operations


1. SQL merge
Posting a tweet simply inserts the new tweet into a global collection of tweets.
When a user requests their home timeline, simply look up all the people they follow, 
find all tweets for each of those users, merge them sorted by time.

```sql
    SELECT tweets.*, users.* FROM tweets
    JOIN users ON tweets.sender_id = users.id
    JOIN follows ON follows.followee_id = users.id
    WHERE follows.follower_id = current_user
```

2. User Cache Timeline
Maintain a home timeline cache for each user, a mailbox of tweets for each user.
When a user posts a tweet, look up all people who follow that user (their mail
recipients or followees), and insert the new tweet into each of their home 
timeline caches. The request to read home timeline is cheap because its been
computed ahead of time.

> Twitter IRL

The first version of Twitter used approach 1.
However, systems struggled to keep up so they switched to approach 2.

It ended up performing better as the average rate of published tweets is almost
two orders of magnitude lower than the rate of home timeline reads.
So its preferable to do more work at write time and less at read.

The downside of approach 2 is posting a tweet now requires more work.
On average, a tweet is delivered to about 75 followers.
So 4.6k tweets per second becomes 345k writes per second to user's home timeline
caches. 

> Twitter Celebrities

Twitter also moved to a hybrid of both approaches. Most user's tweets are fanned
out to timelines at the time when they are posted, but small number of users with
very larger number of followers, 'celebrities', are excepted from this. 

Some users have over 30 millions followers, which means a single tweet may result
in over 30 million writes to timelines, and as twitter tries to deliver tweets
to followers within five seconds, this creates a significant challenge.

Thus, and tweets from celebrities are fetched separately and merged
with that users' home timeline when it is read, like in approach 1.

We will revisit this example in Chapter 12 after having covered more technical ground!

## Describing Performance

Once you have described load and load parameters on your architecture and system,
you can investigate what happens when load increases.

- When you increase a load parameter and keep system resources unchanged, how
is the performance of your system affected?

- Wne you increase a load parameter, how much do you need to increase the resources
if you want to keep performance unchanged

Again, to determine this requires performance numbers, so how to describe it?

## Maintainability

> Specifically over time, different people will build, work, and add to the system 
> (engineering and operations, both maintaining behavior and adapting the system),
> and they should all be able to work on it *productively* 
> (obviously ignoring the unavoidable knife fights over code comment style)

## Intro


